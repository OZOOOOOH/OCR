Figure 2, Model Scaling. (a) is a baseline network example; (b)-(d) are conventional scaling that only increases one dimension of network
width, depth, or resolution. (e) is our proposed compound scaling method that uniformly scales all three dimensions with a fixed ratio.

and resolution with a set of fixed scaling coefficients. For
example, if we want to use 2" times more computational
Tesources, then we can simply increase the network depth by
a¡¯, width by 8¡±, and image size by -y", where a, 8, y are
constant coefficients determined by a small grid search on
the original small model. Figure 2 illustrates the difference
between our scaling method and conventional methods.

Intuitively, the compound scaling method makes sense be-
cause if the input image is bigger, then the network needs
more layers to increase the receptive field and more channels
to capture more fine-grained patterns on the bigger image. In
fact, previous theoretical (Raghu et al., 2017; Lu et al., 2018)
and empirical results (Zagoruyko & Komodakis, 2016) both
show that there exists certain relationship between network
width and depth, but to our best knowledge, we are the
first to empirically quantify the relationship among all three
dimensions of network width, depth, and resolution.

¡®We demonstrate that our scaling method work well on exist-
ing MobileNets (Howard et al., 2017; Sandler et al., 2018)
and ResNet (He et al., 2016). Notably, the effectiveness of
model scaling heavily depends on the baseline network; to
go even further, we use neural architecture search (Zoph
& Le, 2017; Tan et al., 2019) to develop a new baseline
network, and scale it up to obtain a family of models, called
EfficientNets. Figure 1 summarizes the ImageNet perfor-
mance, where our EfficientNets significantly outperform
other ConvNets. In particular, our EfficientNet-B7 surpasses
the best existing GPipe accuracy (Huang et al., 2018), but
using 8.4x fewer parameters and running 6.1x faster on in-
ference. Compared to the widely used ResNet-50 (He et al.,
2016), our EfficientNet-B4 improves the top-1 accuracy
from 76.3% to 83.0% (+6.7%) with similar FLOPS. Besides
ImageNet, EfficientNets also transfer well and achieve state-

of-the-art accuracy on 5 out of 8 widely used datasets, while
reducing parameters by up to 21x than existing ConvNets.

2. Related Work

ConyNet Accuracy: Since AlexNet (Krizhevsky et al.,
2012) won the 2012 ImageNet competition, ConvNets have
become increasingly more accurate by going bigger: while
the 2014 ImageNet winner GoogleNet (Szegedy et al., 2015)
achieves 74.8% top-1 accuracy with about 6.8M parameters,
the 2017 ImageNet winner SENet (Hu et al., 2018) achieves
82.7% top-1 accuracy with 145M parameters. Recently,
GPipe (Huang et al., 2018) further pushes the state-of-the-art
ImageNet top-1 validation accuracy to 84.3% using 557M
parameters: it is so big that it can only be trained with a
specialized pipeline parallelism library by partitioning the
network and spreading each part to a different accelera-
tor. While these models are mainly designed for ImageNet,
tecent studies have shown better ImageNet models also per-
form better across a variety of transfer learning datasets
(Kornblith et al., 2019), and other computer vision tasks
such as object detection (He et al., 2016; Tan et al., 2019).
Although higher accuracy is critical for many applications,
we have already hit the hardware memory limit, and thus
further accuracy gain needs better efficiency.

ConvNet Efficiency: Deep ConvNets are often over-
parameterized. Model compression (Han et al., 2016; He
et al., 2018; Yang et al., 2018) is a common way to re-
duce model size by trading accuracy for efficiency. As mo-
bile phones become ubiquitous, it is also common to hand-
craft efficient mobile-size ConvNets, such as SqueezeNets
(Iandola et al., 2016; Gholami et al., 2018), MobileNets
(Howard et al., 2017; Sandler et al., 2018), and ShuffleNets
